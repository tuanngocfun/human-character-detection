{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/5c/e9/ee572691a3fb05555bcde41826faad29ae4bc1fb07982e7f53d54a176879/scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ngoc/anaconda3/envs/labelImg/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ngoc/anaconda3/envs/labelImg/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m318.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m482.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "\u001b[33mDEPRECATION: wfuzz 3.1.0 has a non-standard dependency specifier pyparsing>=2.4*; python_version >= \"3.5\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of wfuzz or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_v1:\n",
      "- with .yaml files: 1\n",
      "data_v1/test/labels:\n",
      "- with .txt files: 535\n",
      "data_v1/test/images:\n",
      "- with .jpg files: 535\n",
      "data_v1/valid:\n",
      "- with .cache files: 1\n",
      "data_v1/valid/labels:\n",
      "- with .txt files: 1129\n",
      "data_v1/valid/images:\n",
      "- with .jpg files: 1129\n",
      "data_v1/train:\n",
      "- with .cache files: 1\n",
      "data_v1/train/labels:\n",
      "- with .txt files: 7677\n",
      "data_v1/train/images:\n",
      "- with .jpg files: 7677\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files_with_same_extension(directory):\n",
    "    extension_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                extension_count[root][file_extension] += 1\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied: {os.path.join(root, file)}, skipping this file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}. Skipping this file.\")\n",
    "    return extension_count\n",
    "\n",
    "def print_extension_counts(extension_count):\n",
    "    for dir_path, extensions in extension_count.items():\n",
    "        print(dir_path + \":\")\n",
    "        for ext, count in extensions.items():\n",
    "            print(f\"- with {ext} files: {count}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        directory = 'data_v1'\n",
    "        if not os.path.exists(directory):\n",
    "            print(\"Invalid directory path. Please ensure the directory exists.\")\n",
    "            return\n",
    "        extension_count = count_files_with_same_extension(directory)\n",
    "        print_extension_counts(extension_count)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter-frame-similarity-groups:\n",
      "- with .jpg files: 7477\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files_with_same_extension(directory):\n",
    "    extension_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                extension_count[root][file_extension] += 1\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied: {os.path.join(root, file)}, skipping this file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}. Skipping this file.\")\n",
    "    return extension_count\n",
    "\n",
    "def print_extension_counts(extension_count):\n",
    "    for dir_path, extensions in extension_count.items():\n",
    "        print(dir_path + \":\")\n",
    "        for ext, count in extensions.items():\n",
    "            print(f\"- with {ext} files: {count}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        directory = 'filter-frame-similarity-groups'\n",
    "        if not os.path.exists(directory):\n",
    "            print(\"Invalid directory path. Please ensure the directory exists.\")\n",
    "            return\n",
    "        extension_count = count_files_with_same_extension(directory)\n",
    "        print_extension_counts(extension_count)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 1707 .jpg files from filter-frame-similarity-groups/ to filter-frame/\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "source_directory = 'filter-frame-similarity-groups/'\n",
    "target_directory = 'filter-frame/'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "jpg_files = glob.glob(f'{source_directory}*.jpg')\n",
    "\n",
    "for file in jpg_files:\n",
    "    shutil.move(file, target_directory)\n",
    "\n",
    "print(f\"Moved {len(jpg_files)} .jpg files from {source_directory} to {target_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(image_vectors):\n\u001b[1;32m     38\u001b[0m     vector \u001b[39m=\u001b[39m image_vectors[i]\n\u001b[0;32m---> 39\u001b[0m     similarity \u001b[39m=\u001b[39m cosine_similarity([first_image], [vector])\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold:\n\u001b[1;32m     41\u001b[0m         group\u001b[39m.\u001b[39mappend(image_paths\u001b[39m.\u001b[39mpop(i))\n",
      "File \u001b[0;32m~/anaconda3/envs/labelImg/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/labelImg/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1577\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m \n\u001b[1;32m   1544\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1577\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1579\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/anaconda3/envs/labelImg/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:165\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    156\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    157\u001b[0m         X,\n\u001b[1;32m    158\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    166\u001b[0m         X,\n\u001b[1;32m    167\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    168\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    169\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    170\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    171\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    174\u001b[0m         Y,\n\u001b[1;32m    175\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m~/anaconda3/envs/labelImg/lib/python3.10/site-packages/sklearn/utils/validation.py:824\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m         dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(dtype, (\u001b[39mlist\u001b[39;49m, \u001b[39mtuple\u001b[39;49m)):\n\u001b[1;32m    825\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dtype_orig \u001b[39min\u001b[39;00m dtype:\n\u001b[1;32m    826\u001b[0m         \u001b[39m# no dtype conversion required\u001b[39;00m\n\u001b[1;32m    827\u001b[0m         dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories\n",
    "source_dir = '/media/ngoc/mydisk/ngoc/thesis/data/filter-frame'\n",
    "target_dir = '/media/ngoc/mydisk/ngoc/thesis/data/filter-frame-similarity-groups'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Function to convert image to vector\n",
    "def image_to_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((100, 100)) # resizing the image\n",
    "    return np.array(image).flatten()\n",
    "\n",
    "# Read images and convert to vectors\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        image_paths.append(image_path)\n",
    "        image_vectors.append(image_to_vector(image_path))\n",
    "\n",
    "# Calculate similarities and group images\n",
    "threshold = 0.9 # similarity threshold\n",
    "groups = []\n",
    "while image_vectors:\n",
    "    first_image = image_vectors.pop(0)\n",
    "    group = [image_paths.pop(0)]\n",
    "    i = 0\n",
    "    while i < len(image_vectors):\n",
    "        vector = image_vectors[i]\n",
    "        similarity = cosine_similarity([first_image], [vector])\n",
    "        if similarity >= threshold:\n",
    "            group.append(image_paths.pop(i))\n",
    "            image_vectors.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    groups.append(group)\n",
    "\n",
    "# Move the similar images to the target directory, retaining the first in each group\n",
    "for group in groups:\n",
    "    for image_path in group[1:]: # Skip the first image in the group\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cpu resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m groups \u001b[39m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor() \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mfor\u001b[39;00m i, result \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(executor\u001b[39m.\u001b[39mmap(find_similar_images, image_vectors, [image_vectors]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(image_vectors), [image_paths]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(image_paths))):\n\u001b[1;32m     44\u001b[0m         groups\u001b[39m.\u001b[39mappend([image_paths[i]] \u001b[39m+\u001b[39m result)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Move the similar images to the target directory, retaining the first in each group\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories\n",
    "source_dir = 'filter-frame'\n",
    "target_dir = 'filter-frame-similarity-groups'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Function to convert image to vector\n",
    "def image_to_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((100, 100)) # resizing the image\n",
    "    return np.array(image).flatten()\n",
    "\n",
    "# Read images and convert to vectors\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        image_paths.append(image_path)\n",
    "        image_vectors.append(image_to_vector(image_path))\n",
    "\n",
    "# Function to find similar images\n",
    "def find_similar_images(first_image, vectors, paths, threshold=0.9):\n",
    "    group = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        similarity = cosine_similarity([first_image], [vector])\n",
    "        if similarity >= threshold:\n",
    "            group.append(paths[i])\n",
    "    return group\n",
    "\n",
    "# Parallelize finding similar images using all CPU cores\n",
    "groups = []\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for i, result in enumerate(executor.map(find_similar_images, image_vectors, [image_vectors]*len(image_vectors), [image_paths]*len(image_paths))):\n",
    "        groups.append([image_paths[i]] + result)\n",
    "\n",
    "# Move the similar images to the target directory, retaining the first in each group\n",
    "for group in groups:\n",
    "    for image_path in group[1:]: # Skip the first image in the group\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape the vector to 2d, skip the first image in similarity group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m groups \u001b[39m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mfor\u001b[39;00m i, result \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(executor\u001b[39m.\u001b[39mmap(find_similar_images, image_vectors, [image_vectors]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(image_vectors), [image_paths]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(image_paths))):\n\u001b[1;32m     39\u001b[0m         groups\u001b[39m.\u001b[39mappend([image_paths[i]] \u001b[39m+\u001b[39m result)\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m groups:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "source_dir = 'filter-frame'\n",
    "target_dir = 'filter-frame-similarity-groups'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def image_to_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((100, 100))\n",
    "    return np.array(image).flatten().reshape(1, -1) # Reshape the vector\n",
    "\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        image_paths.append(image_path)\n",
    "        image_vectors.append(image_to_vector(image_path))\n",
    "\n",
    "def find_similar_images(first_image, vectors, paths, threshold=0.9):\n",
    "    group = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        if i == 0: continue # Skip the first image itself\n",
    "        similarity = cosine_similarity(first_image, vector) # Directly use the reshaped vector\n",
    "        if similarity[0][0] >= threshold:\n",
    "            group.append(paths[i])\n",
    "    return group\n",
    "\n",
    "groups = []\n",
    "with ProcessPoolExecutor(max_workers=24) as executor:\n",
    "    for i, result in enumerate(executor.map(find_similar_images, image_vectors, [image_vectors]*len(image_vectors), [image_paths]*len(image_paths))):\n",
    "        groups.append([image_paths[i]] + result)\n",
    "\n",
    "for group in groups:\n",
    "    for image_path in group[1:]: # Start from the second image in the group\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "                print(f\"Successfully moved similar image: {image_path}\") # Logging\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to move {image_path}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {image_path}\")\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m [v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vectors \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     34\u001b[0m image_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(source_dir, filename) \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(source_dir) \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m---> 35\u001b[0m image_vectors \u001b[39m=\u001b[39m process_images(image_paths)\n\u001b[1;32m     37\u001b[0m similarities \u001b[39m=\u001b[39m cosine_similarity(image_vectors)\n\u001b[1;32m     38\u001b[0m processed_images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(image_paths), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_images\u001b[39m(image_paths):\n\u001b[1;32m     30\u001b[0m     \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39mmax_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 31\u001b[0m         vectors \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(executor\u001b[39m.\u001b[39;49mmap(image_to_vector, image_paths))\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m [v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vectors \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/process.py:766\u001b[0m, in \u001b[0;36mProcessPoolExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mchunksize must be >= 1.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 766\u001b[0m results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mmap(partial(_process_chunk, fn),\n\u001b[1;32m    767\u001b[0m                       _get_chunks(\u001b[39m*\u001b[39;49miterables, chunksize\u001b[39m=\u001b[39;49mchunksize),\n\u001b[1;32m    768\u001b[0m                       timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    769\u001b[0m \u001b[39mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:610\u001b[0m, in \u001b[0;36mExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     end_time \u001b[39m=\u001b[39m timeout \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 610\u001b[0m fs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmit(fn, \u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39miterables)]\n\u001b[1;32m    612\u001b[0m \u001b[39m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/_base.py:610\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     end_time \u001b[39m=\u001b[39m timeout \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 610\u001b[0m fs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubmit(fn, \u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39miterables)]\n\u001b[1;32m    612\u001b[0m \u001b[39m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/process.py:734\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[39m# Wake up queue management thread\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor_manager_thread_wakeup\u001b[39m.\u001b[39;49mwakeup()\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_safe_to_dynamically_spawn_children:\n\u001b[1;32m    737\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adjust_process_count()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/concurrent/futures/process.py:79\u001b[0m, in \u001b[0;36m_ThreadWakeup.wakeup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwakeup\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writer\u001b[39m.\u001b[39;49msend_bytes(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/multiprocessing/connection.py:200\u001b[0m, in \u001b[0;36m_ConnectionBase.send_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39melif\u001b[39;00m offset \u001b[39m+\u001b[39m size \u001b[39m>\u001b[39m n:\n\u001b[1;32m    199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbuffer length < offset + size\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_bytes(m[offset:offset \u001b[39m+\u001b[39;49m size])\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[39m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send(header \u001b[39m+\u001b[39;49m buf)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[39m=\u001b[39m write(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, buf)\n\u001b[1;32m    369\u001b[0m     remaining \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m remaining \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "source_dir = '../../filter-frame3'\n",
    "target_dir = '../../filter-frame3-similarity-groups'\n",
    "threshold = 0.9\n",
    "image_size = (100, 100)\n",
    "max_workers = 24\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def image_to_vector(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path).convert('RGB') as image:\n",
    "            image = image.resize(image_size)\n",
    "            return np.array(image).flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_images(image_paths):\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        vectors = list(executor.map(image_to_vector, image_paths))\n",
    "    return [v for v in vectors if v is not None]\n",
    "\n",
    "image_paths = [os.path.join(source_dir, filename) for filename in os.listdir(source_dir) if filename.endswith(\".jpg\")]\n",
    "image_vectors = process_images(image_paths)\n",
    "\n",
    "similarities = cosine_similarity(image_vectors)\n",
    "processed_images = np.zeros(len(image_paths), dtype=bool)\n",
    "\n",
    "for i in range(len(image_vectors)):\n",
    "    if processed_images[i]:\n",
    "        continue\n",
    "\n",
    "    group = [j for j in range(len(image_vectors)) if similarities[i][j] >= threshold and not processed_images[j]]\n",
    "    for j in group:\n",
    "        processed_images[j] = True\n",
    "\n",
    "    first_image = image_paths[i]\n",
    "    for j in group[1:]:\n",
    "        source_path = image_paths[j]\n",
    "        target_path = os.path.join(target_dir, os.path.basename(source_path))\n",
    "        logging.info(f\"Moving from {source_path} to {target_path}\")\n",
    "        if os.path.exists(source_path):\n",
    "            try:\n",
    "                shutil.move(source_path, target_path)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to move {source_path}: {str(e)}\")\n",
    "        else:\n",
    "            logging.error(f\"Source file not found: {source_path}\")\n",
    "\n",
    "logging.info(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficient code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "source_dir = '../../filter-frame3'\n",
    "target_dir = '../../filter-frame3-similarity-groups'\n",
    "threshold = 0.9\n",
    "image_size = (100, 100)\n",
    "max_workers = 24\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def image_to_vector(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path).convert('RGB') as image:\n",
    "            image = image.resize(image_size)\n",
    "            return np.array(image).flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_images(image_paths):\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        vectors = list(executor.map(image_to_vector, image_paths))\n",
    "    return [v for v in vectors if v is not None]\n",
    "\n",
    "image_paths = [os.path.join(source_dir, filename) for filename in os.listdir(source_dir) if filename.endswith(\".jpg\")]\n",
    "image_vectors = process_images(image_paths)\n",
    "\n",
    "similarities = cosine_similarity(image_vectors)\n",
    "processed_images = np.zeros(len(image_paths), dtype=bool)\n",
    "\n",
    "# Find similar images and move them\n",
    "for i, similar_indices in enumerate((similarities >= threshold) & ~processed_images):\n",
    "    group = np.where(similar_indices)[0]\n",
    "    processed_images[group] = True\n",
    "\n",
    "    for j in group[1:]:\n",
    "        source_path = image_paths[j]\n",
    "        target_path = os.path.join(target_dir, os.path.basename(source_path))\n",
    "        logging.info(f\"Moving from {source_path} to {target_path}\")\n",
    "        if os.path.exists(source_path):\n",
    "            try:\n",
    "                shutil.move(source_path, target_path)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to move {source_path}: {str(e)}\")\n",
    "        else:\n",
    "            logging.error(f\"Source file not found: {source_path}\")\n",
    "\n",
    "logging.info(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files_with_same_extension(directory):\n",
    "    extension_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                extension_count[root][file_extension] += 1\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied: {os.path.join(root, file)}, skipping this file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}. Skipping this file.\")\n",
    "    return extension_count\n",
    "\n",
    "def print_extension_counts(extension_count):\n",
    "    for dir_path, extensions in extension_count.items():\n",
    "        print(dir_path + \":\")\n",
    "        for ext, count in extensions.items():\n",
    "            print(f\"- with {ext} files: {count}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        directory = '../../filter-frame3'\n",
    "        if not os.path.exists(directory):\n",
    "            print(\"Invalid directory path. Please ensure the directory exists.\")\n",
    "            return\n",
    "        extension_count = count_files_with_same_extension(directory)\n",
    "        print_extension_counts(extension_count)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_files_with_same_extension(directory):\n",
    "    extension_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                file_extension = os.path.splitext(file)[1]\n",
    "                extension_count[root][file_extension] += 1\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied: {os.path.join(root, file)}, skipping this file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}. Skipping this file.\")\n",
    "    return extension_count\n",
    "\n",
    "def print_extension_counts(extension_count):\n",
    "    for dir_path, extensions in extension_count.items():\n",
    "        print(dir_path + \":\")\n",
    "        for ext, count in extensions.items():\n",
    "            print(f\"- with {ext} files: {count}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        directory = '../../filter-frame3-similarity-groups'\n",
    "        if not os.path.exists(directory):\n",
    "            print(\"Invalid directory path. Please ensure the directory exists.\")\n",
    "            return\n",
    "        extension_count = count_files_with_same_extension(directory)\n",
    "        print_extension_counts(extension_count)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu then cpu, if gpu is running out of memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(image_vectors):\n\u001b[1;32m     39\u001b[0m     vector \u001b[39m=\u001b[39m image_vectors[i]\n\u001b[0;32m---> 40\u001b[0m     similarity \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreduce_mean(tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mcosine_similarity([first_image], [vector]))\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold:\n\u001b[1;32m     42\u001b[0m         group\u001b[39m.\u001b[39mappend(image_paths\u001b[39m.\u001b[39mpop(i))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2721\u001b[0m, in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2668\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Computes the mean of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[1;32m   2669\u001b[0m \n\u001b[1;32m   2670\u001b[0m \u001b[39mReduces `input_tensor` along the dimensions given in `axis` by computing the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2715\u001b[0m \u001b[39m@end_compatibility\u001b[39;00m\n\u001b[1;32m   2716\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2717\u001b[0m keepdims \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mbool\u001b[39m(keepdims)\n\u001b[1;32m   2718\u001b[0m \u001b[39mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[1;32m   2719\u001b[0m     keepdims, axis,\n\u001b[1;32m   2720\u001b[0m     gen_math_ops\u001b[39m.\u001b[39mmean(\n\u001b[0;32m-> 2721\u001b[0m         input_tensor, _ReductionDims(input_tensor, axis), keepdims,\n\u001b[1;32m   2722\u001b[0m         name\u001b[39m=\u001b[39mname))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2230\u001b[0m, in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[39m# Fast path: avoid creating Rank and Range ops if ndims is known.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m \u001b[39mif\u001b[39;00m x_rank:\n\u001b[0;32m-> 2230\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(np\u001b[39m.\u001b[39;49marange(x_rank, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mint32))\n\u001b[1;32m   2231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2232\u001b[0m   \u001b[39m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, array_ops\u001b[39m.\u001b[39mrank(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    264\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    274\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 275\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    277\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    284\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    286\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directories\n",
    "source_dir = 'filter-frame'\n",
    "target_dir = 'filter-frame-similarity-groups'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Function to convert image to vector\n",
    "def image_to_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((100, 100)) # resizing the image\n",
    "    image_array = tf.convert_to_tensor(image) # Convert to tensor\n",
    "    image_array = tf.image.convert_image_dtype(image_array, tf.float32) # Normalize to [0, 1]\n",
    "    return tf.reshape(image_array, [-1])\n",
    "\n",
    "# Read images and convert to vectors\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        image_paths.append(image_path)\n",
    "        image_vectors.append(image_to_vector(image_path))\n",
    "\n",
    "# Calculate similarities and group images\n",
    "threshold = 0.9 # similarity threshold\n",
    "groups = []\n",
    "while image_vectors:\n",
    "    first_image = image_vectors.pop(0)\n",
    "    group = [image_paths.pop(0)]\n",
    "    i = 0\n",
    "    while i < len(image_vectors):\n",
    "        vector = image_vectors[i]\n",
    "        similarity = tf.reduce_mean(tf.keras.losses.cosine_similarity([first_image], [vector]))\n",
    "        if similarity >= threshold:\n",
    "            group.append(image_paths.pop(i))\n",
    "            image_vectors.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    groups.append(group)\n",
    "\n",
    "# Move the similar images to the target directory, retaining the first in each group\n",
    "for group in groups:\n",
    "    for image_path in group[1:]: # Skip the first image in the group\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 09:57:38.307697: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 09:57:38.329934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 09:57:38.720452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 09:57:39.137243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.151876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.152314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.153370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.154016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.154395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.450514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.450920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.451262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 09:57:39.451601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10817 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(image_vectors):\n\u001b[1;32m     34\u001b[0m     vector \u001b[39m=\u001b[39m image_vectors[i]\n\u001b[0;32m---> 35\u001b[0m     similarity \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mcosine_similarity([first_image], [vector]))\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold:\n\u001b[1;32m     37\u001b[0m         group\u001b[39m.\u001b[39mappend(image_paths\u001b[39m.\u001b[39mpop(i))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/keras/src/losses.py:2738\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(y_true, y_pred, axis)\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Computes the cosine similarity between labels and predictions.\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \n\u001b[1;32m   2710\u001b[0m \u001b[39mNote that it is a number between -1 and 1. When it is a negative number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2735\u001b[0m \u001b[39m  Cosine similarity tensor.\u001b[39;00m\n\u001b[1;32m   2736\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39ml2_normalize(y_true, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m-> 2738\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49ml2_normalize(y_pred, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   2739\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(y_true \u001b[39m*\u001b[39m y_pred, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:576\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    569\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    570\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    571\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    575\u001b[0m           instructions)\n\u001b[0;32m--> 576\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/ops/nn_impl.py:695\u001b[0m, in \u001b[0;36ml2_normalize\u001b[0;34m(x, axis, epsilon, name, dim)\u001b[0m\n\u001b[1;32m    693\u001b[0m axis \u001b[39m=\u001b[39m deprecated_argument_lookup(\u001b[39m\"\u001b[39m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m, axis, \u001b[39m\"\u001b[39m\u001b[39mdim\u001b[39m\u001b[39m\"\u001b[39m, dim)\n\u001b[1;32m    694\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39ml2_normalize\u001b[39m\u001b[39m\"\u001b[39m, [x]) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m--> 695\u001b[0m   x \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(x, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    696\u001b[0m   \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex:\n\u001b[1;32m    697\u001b[0m     square_real \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39msquare(math_ops\u001b[39m.\u001b[39mreal(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1443\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m   1444\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:324\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    323\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 324\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    264\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    274\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 275\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    277\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    284\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    286\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = '../../filter-frame3'\n",
    "target_dir = '../../filter-frame3-similarity-groups'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, (100, 100))\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image_path, tf.reshape(image, [-1])\n",
    "\n",
    "image_paths_ds = tf.data.Dataset.list_files(os.path.join(source_dir, \"*.jpg\"))\n",
    "image_vectors_ds = image_paths_ds.map(load_and_process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for path, vector in image_vectors_ds.as_numpy_iterator():\n",
    "    image_vectors.append(vector)\n",
    "    image_paths.append(path.decode())\n",
    "\n",
    "threshold = 0.9\n",
    "groups = []\n",
    "while image_vectors:\n",
    "    first_image = image_vectors.pop(0)\n",
    "    group = [image_paths.pop(0)]\n",
    "    i = 0\n",
    "    while i < len(image_vectors):\n",
    "        vector = image_vectors[i]\n",
    "        similarity = tf.reduce_mean(tf.keras.losses.cosine_similarity([first_image], [vector]))\n",
    "        if similarity >= threshold:\n",
    "            group.append(image_paths.pop(i))\n",
    "            image_vectors.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    groups.append(group)\n",
    "\n",
    "for group in groups:\n",
    "    for image_path in group[1:]:\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 10:10:08.682380: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 10:10:08.705579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 10:10:09.109841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 10:10:09.538820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.552766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.553213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.554476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.554853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.555198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.838193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.838590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.838933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 10:10:09.839274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10915 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity filtering and grouping is complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Allow GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Case of having multiple GPUs, uncomment the following line to enable MirroredStrategy\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "source_dir = '../../filter-frame3'\n",
    "target_dir = '../../filter-frame3-similarity-groups'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, (100, 100))\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image_path, tf.reshape(image, [-1])\n",
    "\n",
    "image_paths_ds = tf.data.Dataset.list_files(os.path.join(source_dir, \"*.jpg\"))\n",
    "\n",
    "# Increase the number of parallel calls to utilize more workers\n",
    "num_cores = os.cpu_count() # or another number that suits system\n",
    "image_vectors_ds = image_paths_ds.map(load_and_process_image, num_parallel_calls=num_cores)\n",
    "\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for path, vector in image_vectors_ds.as_numpy_iterator():\n",
    "    image_vectors.append(vector)\n",
    "    image_paths.append(path.decode())\n",
    "\n",
    "threshold = 0.9\n",
    "groups = []\n",
    "while image_vectors:\n",
    "    first_image = image_vectors.pop(0)\n",
    "    group = [image_paths.pop(0)]\n",
    "    i = 0\n",
    "    while i < len(image_vectors):\n",
    "        vector = image_vectors[i]\n",
    "        similarity = tf.reduce_mean(tf.keras.losses.cosine_similarity([first_image], [vector]))\n",
    "        if similarity >= threshold:\n",
    "            group.append(image_paths.pop(i))\n",
    "            image_vectors.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    groups.append(group)\n",
    "\n",
    "for group in groups:\n",
    "    for image_path in group[1:]:\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inter op parallelism cannot be modified after initialization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Configure TensorFlow to use all available CPU resources\u001b[39;00m\n\u001b[1;32m      6\u001b[0m num_threads \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mcpu_count() \u001b[39m# Get the number of available CPU cores\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tf\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mthreading\u001b[39m.\u001b[39;49mset_inter_op_parallelism_threads(num_threads)\n\u001b[1;32m      8\u001b[0m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mthreading\u001b[39m.\u001b[39mset_intra_op_parallelism_threads(num_threads)\n\u001b[1;32m     10\u001b[0m source_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfilter-frame\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/framework/config.py:144\u001b[0m, in \u001b[0;36mset_inter_op_parallelism_threads\u001b[0;34m(num_threads)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconfig.threading.set_inter_op_parallelism_threads\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_inter_op_parallelism_threads\u001b[39m(num_threads):\n\u001b[1;32m    136\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Set number of threads used for parallelism between independent operations.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39m  Determines the number of threads used by independent non-blocking operations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m    num_threads: Number of parallel threads\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m   context\u001b[39m.\u001b[39;49mcontext()\u001b[39m.\u001b[39;49minter_op_parallelism_threads \u001b[39m=\u001b[39m num_threads\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1962\u001b[0m, in \u001b[0;36mContext.inter_op_parallelism_threads\u001b[0;34m(self, num_threads)\u001b[0m\n\u001b[1;32m   1959\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context_handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1962\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1963\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mInter op parallelism cannot be modified after initialization.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1965\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inter_op_parallelism_threads \u001b[39m=\u001b[39m num_threads\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inter op parallelism cannot be modified after initialization."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Configure TensorFlow to use all available CPU resources\n",
    "num_threads = os.cpu_count() # Get the number of available CPU cores\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "\n",
    "source_dir = 'filter-frame'\n",
    "target_dir = 'filter-frame-similarity-groups'\n",
    "\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, (100, 100))\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image_path, tf.reshape(image, [-1])\n",
    "\n",
    "image_paths_ds = tf.data.Dataset.list_files(os.path.join(source_dir, \"*.jpg\"))\n",
    "image_vectors_ds = image_paths_ds.map(load_and_process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "image_vectors = []\n",
    "image_paths = []\n",
    "for path, vector in image_vectors_ds.as_numpy_iterator():\n",
    "    image_vectors.append(vector)\n",
    "    image_paths.append(path.decode())\n",
    "\n",
    "threshold = 0.9\n",
    "groups = []\n",
    "while image_vectors:\n",
    "    first_image = image_vectors.pop(0)\n",
    "    group = [image_paths.pop(0)]\n",
    "    i = 0\n",
    "    while i < len(image_vectors):\n",
    "        vector = image_vectors[i]\n",
    "        similarity = tf.reduce_mean(tf.keras.losses.cosine_similarity([first_image], [vector]))\n",
    "        if similarity >= threshold:\n",
    "            group.append(image_paths.pop(i))\n",
    "            image_vectors.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    groups.append(group)\n",
    "\n",
    "for group in groups:\n",
    "    for image_path in group[1:]:\n",
    "        shutil.move(image_path, os.path.join(target_dir, os.path.basename(image_path)))\n",
    "\n",
    "print(\"Similarity filtering and grouping is complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelImg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
